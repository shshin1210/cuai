{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6-1: Softmax Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4836f1acd8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률로 만들어주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytorch has softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z,dim=0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률이기 때문에 총합이 무조건 1이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum() #총합은 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss (Low-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Class 분류에서 cross entropy loss를 loss function으로 사용한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJQAAAApCAYAAADTaAB/AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAPSSURBVHhe7ZbJTSRBEEXbFEzBFCzgzg0vOGIDwgAcwAe8QNxq9BBfE4rJzMoluqGYeFKoujq3WH5G92lLkkBSUEkoKagklBRUEkoKKgklBZWEkoJKQklBJaEcXlAvLy/b9fX15zMZ5+rqaru5ufl6W+fwgnp7e/tMSgpqHHJG7jDyGMGv+MnLDjUHQnp8fNzu7+/DutR/ISglbdSibu1PweZI3Un4LsXnmfj/C0EhjtPp9PksoeSxB3PYT7f30rTiWMF3IB8fcds55IPxUVEtCYrglXwcOlcy9uj5ydsTlYf9Iv+s7sF55BGLxtcGkZAzjz+bdb35EssdiqRTqO+kR1BAwvC199ax5+gNXYHiRQsK/1f2ZO1IDpaVwIGXvMke2jKC6rlJSu5oki7FOQQ102Us1Hbkp39JUBzEjR858LshuSM/fVHQ7fx/Et8BS4JinHWMUVx/ETTOmIpvu3Xtsvf4A9q7lyVBqTjeiRY2+Jox55xQtFG/V6FYdFLiEz5WLyjWlN55Ct6JA2N/3u2epf9K0OMPaF4vS4LCeetQDzi8ZzZhntfX1+35+bnLapB8fFcxZvj4+OgyC3FZIfO0sXpBlfJLcVVgcmXnlzqcf7fs+QN8x5xelgTFQQRxSRAU5+7Z7e3t14oy6q4UZYaHh4d/zvR2d3f3NfsvFFhn+rO9INijNUeCkAiYa7sJYmgJClr+CM7oZVpQHM5BUrel9J0geNa2zN8SC4J6enrqshb4ONLKPe/v713moeNgnO8vY0lQpTm2wFpD3tjX525PDC1/xJ4oLdOCwomaswRXA6cVRM1a66PgnJbwzwWxIeRSjBKHYJ4vpnIkWNO6gKxvxdnyB0Yv3rSgEJN1lqcOv4QgQOIUnM93WAvWtIpwTjiX3JXOx2/GbE55Vzy825yzB+/EQ84xv+9erC1/gO/38mmZEpSKVjMFfG6UcJsMK7AS+NdKsCc6FvbDB48K5/1jPjFhfh1jXGDGeGISmJDQatT8EfKpl+kO9RMgUQRrW3Ir+NHbtifOGVrFHYVYSpfDxihB8iyx5w8CHeHwgiJRBK3E1hLE+IhAmDuazBr4yPnyNwr7cwjszRleZMzx83r8Ydyu6+HQgpJAeGK1BPGdkl8z7YGImOuLtYL2bxVvBuKlO+OrfupqZzBPYz3+MGY7fy+HFRTJxIDgldAS3EYJZ8R+Gy0BeXyX6+WwgvLi4TbVBJVcjkMKituDgOwt4vPsrUriOPR/qOTnkYJKQklBJaGkoJJQUlBJKCmoJJQUVBJKCioJJQWVhJKCSkJJQSWBbNsfaxky92SgrI8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('crossentropyloss.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y 는 실제값, y^은 예측값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5개의 classes와 3개의 sample을 가진 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3,5, requires_grad= True)#3행 5열의 random 2차원 행렬 생성\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5,(3,)).long() #class 정답을 y라고 설정\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답이 0,2,1 이므로 index가 각각 0,2,1인 자리에 1을 부여하고(source) 나머지는 dummy 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dim = 0 아래방향(행)\n",
    "\n",
    "- dim = 1 가로방향(열)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1),1) # scatter_(dim:차원축, index, source)\n",
    "\n",
    "#unsqueeze함수는 squeeze함수의 반대로 1인 차원을 생성하는 함수이다. \n",
    "#그래서 어느 차원에 1인 차원을 생성할 지 꼭 지정해주어야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot* -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy Loss with torch.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has F.log_softmax() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#low level\n",
    "torch.log(F.softmax(z,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#high level\n",
    "F.log_softmax(z,dim=1)   # 바로 log softmax가 되는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also has F.nll_loss() function that computes the negative loss likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#low level\n",
    "\n",
    "(y_one_hot * -torch.log(F.softmax(z,dim=1))).sum(dim=1).mean()  #다 더해주고 평균내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#high level\n",
    "#NLL = negative log likelihood\n",
    "\n",
    "F.nll_loss(F.log_softmax(z,dim=1),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also has F.cross_entropy that combines F.log_softmax() and F.nll_loss().\n",
    "\n",
    "마지막으로 단순화해주는 함수!!\n",
    "\n",
    "(y_one_hot * -torch.log(F.softmax(z,dim=1))).sum(dim=1).mean()==F.cross_entropy(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z,y)  #(y_one_hot * -torch.log(F.softmax(z,dim=1))).sum(dim=1).mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Low-level Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1., 1.],\n",
      "        [2., 1., 3., 2.],\n",
      "        [3., 1., 3., 4.],\n",
      "        [4., 1., 5., 5.],\n",
      "        [1., 7., 5., 5.],\n",
      "        [1., 2., 5., 6.],\n",
      "        [1., 6., 6., 6.],\n",
      "        [1., 7., 7., 7.]])\n"
     ]
    }
   ],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py:149: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630806732/work/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost : 1.098612\n",
      "Epoch  100/1000 Cost : 0.901535\n",
      "Epoch  200/1000 Cost : 0.839114\n",
      "Epoch  300/1000 Cost : 0.807826\n",
      "Epoch  400/1000 Cost : 0.788472\n",
      "Epoch  500/1000 Cost : 0.774822\n",
      "Epoch  600/1000 Cost : 0.764449\n",
      "Epoch  700/1000 Cost : 0.756191\n",
      "Epoch  800/1000 Cost : 0.749398\n",
      "Epoch  900/1000 Cost : 0.743671\n",
      "Epoch 1000/1000 Cost : 0.738749\n"
     ]
    }
   ],
   "source": [
    "#모델 초기화하기\n",
    "W = torch.zeros((4,3), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad= True)\n",
    "\n",
    "#opimizer 설정\n",
    "optimizer = optim.SGD([W,b], lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    #Cost 계산 (1)\n",
    "    hypothesis = F.softmax(x_train.matmul(W)+b, dim =1)\n",
    "    y_one_hot = torch.zeros_like(hypothesis)\n",
    "    y_one_hot.scatter_(1, y_train.unsqueeze(1),1)\n",
    "    cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()\n",
    "    \n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #100번마다 로그 출력하기\n",
    "    if epoch % 100 ==0:\n",
    "        print('Epoch {:4d}/{} Cost : {:.6f}'.format(\n",
    "        epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with F.cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568256\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "#모델 초기화하기\n",
    "W = torch.zeros((4,3), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad= True)\n",
    "\n",
    "#optimizer 설정\n",
    "optimizer = optim.SGD([W,b], lr =0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs +1):\n",
    "    \n",
    "    #Cost 계산하기\n",
    "    z = x_train.matmul(W) +b # hypothesis\n",
    "    cost = F.cross_entropy(z,y_train)\n",
    "    \n",
    "    #cost 로 H(x)개선하기\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #100 번마다 로그 출력하기\n",
    "    if epoch %100 ==0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level Implementation with nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # Output이 3이어야함 (0,1,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost : 1.046181\n",
      "Epoch    2/1000 Cost : 0.929780\n",
      "Epoch    4/1000 Cost : 0.891565\n",
      "Epoch    6/1000 Cost : 0.866827\n",
      "Epoch    8/1000 Cost : 0.859701\n",
      "Epoch   10/1000 Cost : 0.881199\n",
      "Epoch   12/1000 Cost : 0.920918\n",
      "Epoch   14/1000 Cost : 0.935701\n",
      "Epoch   16/1000 Cost : 0.923506\n",
      "Epoch   18/1000 Cost : 0.904911\n",
      "Epoch   20/1000 Cost : 0.887267\n",
      "Epoch   22/1000 Cost : 0.871683\n",
      "Epoch   24/1000 Cost : 0.858027\n",
      "Epoch   26/1000 Cost : 0.845991\n",
      "Epoch   28/1000 Cost : 0.835288\n",
      "Epoch   30/1000 Cost : 0.825683\n",
      "Epoch   32/1000 Cost : 0.816984\n",
      "Epoch   34/1000 Cost : 0.809045\n",
      "Epoch   36/1000 Cost : 0.801748\n",
      "Epoch   38/1000 Cost : 0.794997\n",
      "Epoch   40/1000 Cost : 0.788718\n",
      "Epoch   42/1000 Cost : 0.782850\n",
      "Epoch   44/1000 Cost : 0.777342\n",
      "Epoch   46/1000 Cost : 0.772152\n",
      "Epoch   48/1000 Cost : 0.767245\n",
      "Epoch   50/1000 Cost : 0.762590\n",
      "Epoch   52/1000 Cost : 0.758164\n",
      "Epoch   54/1000 Cost : 0.753942\n",
      "Epoch   56/1000 Cost : 0.749907\n",
      "Epoch   58/1000 Cost : 0.746041\n",
      "Epoch   60/1000 Cost : 0.742330\n",
      "Epoch   62/1000 Cost : 0.738761\n",
      "Epoch   64/1000 Cost : 0.735322\n",
      "Epoch   66/1000 Cost : 0.732003\n",
      "Epoch   68/1000 Cost : 0.728795\n",
      "Epoch   70/1000 Cost : 0.725690\n",
      "Epoch   72/1000 Cost : 0.722681\n",
      "Epoch   74/1000 Cost : 0.719759\n",
      "Epoch   76/1000 Cost : 0.716921\n",
      "Epoch   78/1000 Cost : 0.714160\n",
      "Epoch   80/1000 Cost : 0.711471\n",
      "Epoch   82/1000 Cost : 0.708850\n",
      "Epoch   84/1000 Cost : 0.706292\n",
      "Epoch   86/1000 Cost : 0.703795\n",
      "Epoch   88/1000 Cost : 0.701354\n",
      "Epoch   90/1000 Cost : 0.698966\n",
      "Epoch   92/1000 Cost : 0.696629\n",
      "Epoch   94/1000 Cost : 0.694340\n",
      "Epoch   96/1000 Cost : 0.692096\n",
      "Epoch   98/1000 Cost : 0.689894\n",
      "Epoch  100/1000 Cost : 0.687734\n",
      "Epoch  102/1000 Cost : 0.685612\n",
      "Epoch  104/1000 Cost : 0.683528\n",
      "Epoch  106/1000 Cost : 0.681479\n",
      "Epoch  108/1000 Cost : 0.679464\n",
      "Epoch  110/1000 Cost : 0.677480\n",
      "Epoch  112/1000 Cost : 0.675528\n",
      "Epoch  114/1000 Cost : 0.673605\n",
      "Epoch  116/1000 Cost : 0.671710\n",
      "Epoch  118/1000 Cost : 0.669843\n",
      "Epoch  120/1000 Cost : 0.668001\n",
      "Epoch  122/1000 Cost : 0.666185\n",
      "Epoch  124/1000 Cost : 0.664392\n",
      "Epoch  126/1000 Cost : 0.662622\n",
      "Epoch  128/1000 Cost : 0.660875\n",
      "Epoch  130/1000 Cost : 0.659149\n",
      "Epoch  132/1000 Cost : 0.657444\n",
      "Epoch  134/1000 Cost : 0.655758\n",
      "Epoch  136/1000 Cost : 0.654092\n",
      "Epoch  138/1000 Cost : 0.652444\n",
      "Epoch  140/1000 Cost : 0.650814\n",
      "Epoch  142/1000 Cost : 0.649201\n",
      "Epoch  144/1000 Cost : 0.647605\n",
      "Epoch  146/1000 Cost : 0.646025\n",
      "Epoch  148/1000 Cost : 0.644461\n",
      "Epoch  150/1000 Cost : 0.642912\n",
      "Epoch  152/1000 Cost : 0.641377\n",
      "Epoch  154/1000 Cost : 0.639856\n",
      "Epoch  156/1000 Cost : 0.638350\n",
      "Epoch  158/1000 Cost : 0.636856\n",
      "Epoch  160/1000 Cost : 0.635375\n",
      "Epoch  162/1000 Cost : 0.633907\n",
      "Epoch  164/1000 Cost : 0.632452\n",
      "Epoch  166/1000 Cost : 0.631007\n",
      "Epoch  168/1000 Cost : 0.629574\n",
      "Epoch  170/1000 Cost : 0.628153\n",
      "Epoch  172/1000 Cost : 0.626742\n",
      "Epoch  174/1000 Cost : 0.625341\n",
      "Epoch  176/1000 Cost : 0.623951\n",
      "Epoch  178/1000 Cost : 0.622571\n",
      "Epoch  180/1000 Cost : 0.621200\n",
      "Epoch  182/1000 Cost : 0.619839\n",
      "Epoch  184/1000 Cost : 0.618487\n",
      "Epoch  186/1000 Cost : 0.617144\n",
      "Epoch  188/1000 Cost : 0.615809\n",
      "Epoch  190/1000 Cost : 0.614483\n",
      "Epoch  192/1000 Cost : 0.613165\n",
      "Epoch  194/1000 Cost : 0.611855\n",
      "Epoch  196/1000 Cost : 0.610553\n",
      "Epoch  198/1000 Cost : 0.609258\n",
      "Epoch  200/1000 Cost : 0.607971\n",
      "Epoch  202/1000 Cost : 0.606691\n",
      "Epoch  204/1000 Cost : 0.605418\n",
      "Epoch  206/1000 Cost : 0.604153\n",
      "Epoch  208/1000 Cost : 0.602893\n",
      "Epoch  210/1000 Cost : 0.601640\n",
      "Epoch  212/1000 Cost : 0.600394\n",
      "Epoch  214/1000 Cost : 0.599154\n",
      "Epoch  216/1000 Cost : 0.597920\n",
      "Epoch  218/1000 Cost : 0.596692\n",
      "Epoch  220/1000 Cost : 0.595469\n",
      "Epoch  222/1000 Cost : 0.594252\n",
      "Epoch  224/1000 Cost : 0.593042\n",
      "Epoch  226/1000 Cost : 0.591836\n",
      "Epoch  228/1000 Cost : 0.590635\n",
      "Epoch  230/1000 Cost : 0.589440\n",
      "Epoch  232/1000 Cost : 0.588250\n",
      "Epoch  234/1000 Cost : 0.587065\n",
      "Epoch  236/1000 Cost : 0.585885\n",
      "Epoch  238/1000 Cost : 0.584709\n",
      "Epoch  240/1000 Cost : 0.583538\n",
      "Epoch  242/1000 Cost : 0.582371\n",
      "Epoch  244/1000 Cost : 0.581209\n",
      "Epoch  246/1000 Cost : 0.580052\n",
      "Epoch  248/1000 Cost : 0.578898\n",
      "Epoch  250/1000 Cost : 0.577749\n",
      "Epoch  252/1000 Cost : 0.576604\n",
      "Epoch  254/1000 Cost : 0.575462\n",
      "Epoch  256/1000 Cost : 0.574325\n",
      "Epoch  258/1000 Cost : 0.573192\n",
      "Epoch  260/1000 Cost : 0.572062\n",
      "Epoch  262/1000 Cost : 0.570936\n",
      "Epoch  264/1000 Cost : 0.569813\n",
      "Epoch  266/1000 Cost : 0.568694\n",
      "Epoch  268/1000 Cost : 0.567579\n",
      "Epoch  270/1000 Cost : 0.566467\n",
      "Epoch  272/1000 Cost : 0.565358\n",
      "Epoch  274/1000 Cost : 0.564253\n",
      "Epoch  276/1000 Cost : 0.563150\n",
      "Epoch  278/1000 Cost : 0.562051\n",
      "Epoch  280/1000 Cost : 0.560955\n",
      "Epoch  282/1000 Cost : 0.559862\n",
      "Epoch  284/1000 Cost : 0.558771\n",
      "Epoch  286/1000 Cost : 0.557684\n",
      "Epoch  288/1000 Cost : 0.556600\n",
      "Epoch  290/1000 Cost : 0.555518\n",
      "Epoch  292/1000 Cost : 0.554439\n",
      "Epoch  294/1000 Cost : 0.553363\n",
      "Epoch  296/1000 Cost : 0.552289\n",
      "Epoch  298/1000 Cost : 0.551218\n",
      "Epoch  300/1000 Cost : 0.550150\n",
      "Epoch  302/1000 Cost : 0.549084\n",
      "Epoch  304/1000 Cost : 0.548020\n",
      "Epoch  306/1000 Cost : 0.546959\n",
      "Epoch  308/1000 Cost : 0.545900\n",
      "Epoch  310/1000 Cost : 0.544844\n",
      "Epoch  312/1000 Cost : 0.543790\n",
      "Epoch  314/1000 Cost : 0.542737\n",
      "Epoch  316/1000 Cost : 0.541688\n",
      "Epoch  318/1000 Cost : 0.540640\n",
      "Epoch  320/1000 Cost : 0.539594\n",
      "Epoch  322/1000 Cost : 0.538551\n",
      "Epoch  324/1000 Cost : 0.537510\n",
      "Epoch  326/1000 Cost : 0.536470\n",
      "Epoch  328/1000 Cost : 0.535433\n",
      "Epoch  330/1000 Cost : 0.534397\n",
      "Epoch  332/1000 Cost : 0.533364\n",
      "Epoch  334/1000 Cost : 0.532332\n",
      "Epoch  336/1000 Cost : 0.531302\n",
      "Epoch  338/1000 Cost : 0.530274\n",
      "Epoch  340/1000 Cost : 0.529248\n",
      "Epoch  342/1000 Cost : 0.528223\n",
      "Epoch  344/1000 Cost : 0.527200\n",
      "Epoch  346/1000 Cost : 0.526179\n",
      "Epoch  348/1000 Cost : 0.525160\n",
      "Epoch  350/1000 Cost : 0.524142\n",
      "Epoch  352/1000 Cost : 0.523126\n",
      "Epoch  354/1000 Cost : 0.522111\n",
      "Epoch  356/1000 Cost : 0.521098\n",
      "Epoch  358/1000 Cost : 0.520087\n",
      "Epoch  360/1000 Cost : 0.519077\n",
      "Epoch  362/1000 Cost : 0.518068\n",
      "Epoch  364/1000 Cost : 0.517061\n",
      "Epoch  366/1000 Cost : 0.516055\n",
      "Epoch  368/1000 Cost : 0.515051\n",
      "Epoch  370/1000 Cost : 0.514048\n",
      "Epoch  372/1000 Cost : 0.513047\n",
      "Epoch  374/1000 Cost : 0.512046\n",
      "Epoch  376/1000 Cost : 0.511048\n",
      "Epoch  378/1000 Cost : 0.510050\n",
      "Epoch  380/1000 Cost : 0.509054\n",
      "Epoch  382/1000 Cost : 0.508059\n",
      "Epoch  384/1000 Cost : 0.507065\n",
      "Epoch  386/1000 Cost : 0.506072\n",
      "Epoch  388/1000 Cost : 0.505081\n",
      "Epoch  390/1000 Cost : 0.504091\n",
      "Epoch  392/1000 Cost : 0.503101\n",
      "Epoch  394/1000 Cost : 0.502114\n",
      "Epoch  396/1000 Cost : 0.501127\n",
      "Epoch  398/1000 Cost : 0.500141\n",
      "Epoch  400/1000 Cost : 0.499156\n",
      "Epoch  402/1000 Cost : 0.498173\n",
      "Epoch  404/1000 Cost : 0.497191\n",
      "Epoch  406/1000 Cost : 0.496209\n",
      "Epoch  408/1000 Cost : 0.495229\n",
      "Epoch  410/1000 Cost : 0.494249\n",
      "Epoch  412/1000 Cost : 0.493271\n",
      "Epoch  414/1000 Cost : 0.492293\n",
      "Epoch  416/1000 Cost : 0.491317\n",
      "Epoch  418/1000 Cost : 0.490341\n",
      "Epoch  420/1000 Cost : 0.489367\n",
      "Epoch  422/1000 Cost : 0.488393\n",
      "Epoch  424/1000 Cost : 0.487420\n",
      "Epoch  426/1000 Cost : 0.486448\n",
      "Epoch  428/1000 Cost : 0.485477\n",
      "Epoch  430/1000 Cost : 0.484507\n",
      "Epoch  432/1000 Cost : 0.483538\n",
      "Epoch  434/1000 Cost : 0.482569\n",
      "Epoch  436/1000 Cost : 0.481601\n",
      "Epoch  438/1000 Cost : 0.480634\n",
      "Epoch  440/1000 Cost : 0.479668\n",
      "Epoch  442/1000 Cost : 0.478703\n",
      "Epoch  444/1000 Cost : 0.477738\n",
      "Epoch  446/1000 Cost : 0.476775\n",
      "Epoch  448/1000 Cost : 0.475812\n",
      "Epoch  450/1000 Cost : 0.474850\n",
      "Epoch  452/1000 Cost : 0.473888\n",
      "Epoch  454/1000 Cost : 0.472927\n",
      "Epoch  456/1000 Cost : 0.471967\n",
      "Epoch  458/1000 Cost : 0.471007\n",
      "Epoch  460/1000 Cost : 0.470048\n",
      "Epoch  462/1000 Cost : 0.469090\n",
      "Epoch  464/1000 Cost : 0.468133\n",
      "Epoch  466/1000 Cost : 0.467176\n",
      "Epoch  468/1000 Cost : 0.466220\n",
      "Epoch  470/1000 Cost : 0.465264\n",
      "Epoch  472/1000 Cost : 0.464309\n",
      "Epoch  474/1000 Cost : 0.463355\n",
      "Epoch  476/1000 Cost : 0.462401\n",
      "Epoch  478/1000 Cost : 0.461448\n",
      "Epoch  480/1000 Cost : 0.460495\n",
      "Epoch  482/1000 Cost : 0.459543\n",
      "Epoch  484/1000 Cost : 0.458592\n",
      "Epoch  486/1000 Cost : 0.457641\n",
      "Epoch  488/1000 Cost : 0.456691\n",
      "Epoch  490/1000 Cost : 0.455741\n",
      "Epoch  492/1000 Cost : 0.454792\n",
      "Epoch  494/1000 Cost : 0.453843\n",
      "Epoch  496/1000 Cost : 0.452895\n",
      "Epoch  498/1000 Cost : 0.451947\n",
      "Epoch  500/1000 Cost : 0.451000\n",
      "Epoch  502/1000 Cost : 0.450053\n",
      "Epoch  504/1000 Cost : 0.449107\n",
      "Epoch  506/1000 Cost : 0.448161\n",
      "Epoch  508/1000 Cost : 0.447216\n",
      "Epoch  510/1000 Cost : 0.446271\n",
      "Epoch  512/1000 Cost : 0.445326\n",
      "Epoch  514/1000 Cost : 0.444382\n",
      "Epoch  516/1000 Cost : 0.443439\n",
      "Epoch  518/1000 Cost : 0.442496\n",
      "Epoch  520/1000 Cost : 0.441553\n",
      "Epoch  522/1000 Cost : 0.440611\n",
      "Epoch  524/1000 Cost : 0.439669\n",
      "Epoch  526/1000 Cost : 0.438727\n",
      "Epoch  528/1000 Cost : 0.437786\n",
      "Epoch  530/1000 Cost : 0.436845\n",
      "Epoch  532/1000 Cost : 0.435905\n",
      "Epoch  534/1000 Cost : 0.434965\n",
      "Epoch  536/1000 Cost : 0.434026\n",
      "Epoch  538/1000 Cost : 0.433086\n",
      "Epoch  540/1000 Cost : 0.432147\n",
      "Epoch  542/1000 Cost : 0.431209\n",
      "Epoch  544/1000 Cost : 0.430271\n",
      "Epoch  546/1000 Cost : 0.429333\n",
      "Epoch  548/1000 Cost : 0.428395\n",
      "Epoch  550/1000 Cost : 0.427458\n",
      "Epoch  552/1000 Cost : 0.426521\n",
      "Epoch  554/1000 Cost : 0.425585\n",
      "Epoch  556/1000 Cost : 0.424648\n",
      "Epoch  558/1000 Cost : 0.423713\n",
      "Epoch  560/1000 Cost : 0.422777\n",
      "Epoch  562/1000 Cost : 0.421842\n",
      "Epoch  564/1000 Cost : 0.420907\n",
      "Epoch  566/1000 Cost : 0.419972\n",
      "Epoch  568/1000 Cost : 0.419037\n",
      "Epoch  570/1000 Cost : 0.418103\n",
      "Epoch  572/1000 Cost : 0.417169\n",
      "Epoch  574/1000 Cost : 0.416236\n",
      "Epoch  576/1000 Cost : 0.415302\n",
      "Epoch  578/1000 Cost : 0.414369\n",
      "Epoch  580/1000 Cost : 0.413436\n",
      "Epoch  582/1000 Cost : 0.412504\n",
      "Epoch  584/1000 Cost : 0.411571\n",
      "Epoch  586/1000 Cost : 0.410639\n",
      "Epoch  588/1000 Cost : 0.409707\n",
      "Epoch  590/1000 Cost : 0.408775\n",
      "Epoch  592/1000 Cost : 0.407844\n",
      "Epoch  594/1000 Cost : 0.406913\n",
      "Epoch  596/1000 Cost : 0.405982\n",
      "Epoch  598/1000 Cost : 0.405051\n",
      "Epoch  600/1000 Cost : 0.404121\n",
      "Epoch  602/1000 Cost : 0.403190\n",
      "Epoch  604/1000 Cost : 0.402260\n",
      "Epoch  606/1000 Cost : 0.401330\n",
      "Epoch  608/1000 Cost : 0.400400\n",
      "Epoch  610/1000 Cost : 0.399471\n",
      "Epoch  612/1000 Cost : 0.398541\n",
      "Epoch  614/1000 Cost : 0.397612\n",
      "Epoch  616/1000 Cost : 0.396683\n",
      "Epoch  618/1000 Cost : 0.395754\n",
      "Epoch  620/1000 Cost : 0.394826\n",
      "Epoch  622/1000 Cost : 0.393897\n",
      "Epoch  624/1000 Cost : 0.392969\n",
      "Epoch  626/1000 Cost : 0.392041\n",
      "Epoch  628/1000 Cost : 0.391113\n",
      "Epoch  630/1000 Cost : 0.390186\n",
      "Epoch  632/1000 Cost : 0.389258\n",
      "Epoch  634/1000 Cost : 0.388330\n",
      "Epoch  636/1000 Cost : 0.387403\n",
      "Epoch  638/1000 Cost : 0.386476\n",
      "Epoch  640/1000 Cost : 0.385549\n",
      "Epoch  642/1000 Cost : 0.384622\n",
      "Epoch  644/1000 Cost : 0.383696\n",
      "Epoch  646/1000 Cost : 0.382769\n",
      "Epoch  648/1000 Cost : 0.381843\n",
      "Epoch  650/1000 Cost : 0.380917\n",
      "Epoch  652/1000 Cost : 0.379991\n",
      "Epoch  654/1000 Cost : 0.379065\n",
      "Epoch  656/1000 Cost : 0.378139\n",
      "Epoch  658/1000 Cost : 0.377214\n",
      "Epoch  660/1000 Cost : 0.376288\n",
      "Epoch  662/1000 Cost : 0.375363\n",
      "Epoch  664/1000 Cost : 0.374438\n",
      "Epoch  666/1000 Cost : 0.373513\n",
      "Epoch  668/1000 Cost : 0.372588\n",
      "Epoch  670/1000 Cost : 0.371664\n",
      "Epoch  672/1000 Cost : 0.370739\n",
      "Epoch  674/1000 Cost : 0.369815\n",
      "Epoch  676/1000 Cost : 0.368890\n",
      "Epoch  678/1000 Cost : 0.367966\n",
      "Epoch  680/1000 Cost : 0.367042\n",
      "Epoch  682/1000 Cost : 0.366119\n",
      "Epoch  684/1000 Cost : 0.365195\n",
      "Epoch  686/1000 Cost : 0.364271\n",
      "Epoch  688/1000 Cost : 0.363348\n",
      "Epoch  690/1000 Cost : 0.362425\n",
      "Epoch  692/1000 Cost : 0.361502\n",
      "Epoch  694/1000 Cost : 0.360579\n",
      "Epoch  696/1000 Cost : 0.359656\n",
      "Epoch  698/1000 Cost : 0.358733\n",
      "Epoch  700/1000 Cost : 0.357811\n",
      "Epoch  702/1000 Cost : 0.356888\n",
      "Epoch  704/1000 Cost : 0.355966\n",
      "Epoch  706/1000 Cost : 0.355044\n",
      "Epoch  708/1000 Cost : 0.354122\n",
      "Epoch  710/1000 Cost : 0.353201\n",
      "Epoch  712/1000 Cost : 0.352279\n",
      "Epoch  714/1000 Cost : 0.351358\n",
      "Epoch  716/1000 Cost : 0.350437\n",
      "Epoch  718/1000 Cost : 0.349516\n",
      "Epoch  720/1000 Cost : 0.348595\n",
      "Epoch  722/1000 Cost : 0.347675\n",
      "Epoch  724/1000 Cost : 0.346754\n",
      "Epoch  726/1000 Cost : 0.345834\n",
      "Epoch  728/1000 Cost : 0.344914\n",
      "Epoch  730/1000 Cost : 0.343995\n",
      "Epoch  732/1000 Cost : 0.343075\n",
      "Epoch  734/1000 Cost : 0.342156\n",
      "Epoch  736/1000 Cost : 0.341237\n",
      "Epoch  738/1000 Cost : 0.340318\n",
      "Epoch  740/1000 Cost : 0.339399\n",
      "Epoch  742/1000 Cost : 0.338480\n",
      "Epoch  744/1000 Cost : 0.337562\n",
      "Epoch  746/1000 Cost : 0.336645\n",
      "Epoch  748/1000 Cost : 0.335727\n",
      "Epoch  750/1000 Cost : 0.334810\n",
      "Epoch  752/1000 Cost : 0.333893\n",
      "Epoch  754/1000 Cost : 0.332976\n",
      "Epoch  756/1000 Cost : 0.332060\n",
      "Epoch  758/1000 Cost : 0.331144\n",
      "Epoch  760/1000 Cost : 0.330228\n",
      "Epoch  762/1000 Cost : 0.329313\n",
      "Epoch  764/1000 Cost : 0.328398\n",
      "Epoch  766/1000 Cost : 0.327483\n",
      "Epoch  768/1000 Cost : 0.326569\n",
      "Epoch  770/1000 Cost : 0.325655\n",
      "Epoch  772/1000 Cost : 0.324742\n",
      "Epoch  774/1000 Cost : 0.323829\n",
      "Epoch  776/1000 Cost : 0.322916\n",
      "Epoch  778/1000 Cost : 0.322004\n",
      "Epoch  780/1000 Cost : 0.321092\n",
      "Epoch  782/1000 Cost : 0.320181\n",
      "Epoch  784/1000 Cost : 0.319270\n",
      "Epoch  786/1000 Cost : 0.318360\n",
      "Epoch  788/1000 Cost : 0.317451\n",
      "Epoch  790/1000 Cost : 0.316542\n",
      "Epoch  792/1000 Cost : 0.315633\n",
      "Epoch  794/1000 Cost : 0.314725\n",
      "Epoch  796/1000 Cost : 0.313818\n",
      "Epoch  798/1000 Cost : 0.312911\n",
      "Epoch  800/1000 Cost : 0.312006\n",
      "Epoch  802/1000 Cost : 0.311101\n",
      "Epoch  804/1000 Cost : 0.310196\n",
      "Epoch  806/1000 Cost : 0.309293\n",
      "Epoch  808/1000 Cost : 0.308390\n",
      "Epoch  810/1000 Cost : 0.307488\n",
      "Epoch  812/1000 Cost : 0.306587\n",
      "Epoch  814/1000 Cost : 0.305687\n",
      "Epoch  816/1000 Cost : 0.304787\n",
      "Epoch  818/1000 Cost : 0.303889\n",
      "Epoch  820/1000 Cost : 0.302992\n",
      "Epoch  822/1000 Cost : 0.302095\n",
      "Epoch  824/1000 Cost : 0.301200\n",
      "Epoch  826/1000 Cost : 0.300306\n",
      "Epoch  828/1000 Cost : 0.299414\n",
      "Epoch  830/1000 Cost : 0.298522\n",
      "Epoch  832/1000 Cost : 0.297632\n",
      "Epoch  834/1000 Cost : 0.296743\n",
      "Epoch  836/1000 Cost : 0.295856\n",
      "Epoch  838/1000 Cost : 0.294970\n",
      "Epoch  840/1000 Cost : 0.294085\n",
      "Epoch  842/1000 Cost : 0.293202\n",
      "Epoch  844/1000 Cost : 0.292321\n",
      "Epoch  846/1000 Cost : 0.291442\n",
      "Epoch  848/1000 Cost : 0.290564\n",
      "Epoch  850/1000 Cost : 0.289688\n",
      "Epoch  852/1000 Cost : 0.288814\n",
      "Epoch  854/1000 Cost : 0.287943\n",
      "Epoch  856/1000 Cost : 0.287073\n",
      "Epoch  858/1000 Cost : 0.286206\n",
      "Epoch  860/1000 Cost : 0.285341\n",
      "Epoch  862/1000 Cost : 0.284478\n",
      "Epoch  864/1000 Cost : 0.283618\n",
      "Epoch  866/1000 Cost : 0.282761\n",
      "Epoch  868/1000 Cost : 0.281906\n",
      "Epoch  870/1000 Cost : 0.281054\n",
      "Epoch  872/1000 Cost : 0.280206\n",
      "Epoch  874/1000 Cost : 0.279360\n",
      "Epoch  876/1000 Cost : 0.278518\n",
      "Epoch  878/1000 Cost : 0.277680\n",
      "Epoch  880/1000 Cost : 0.276844\n",
      "Epoch  882/1000 Cost : 0.276013\n",
      "Epoch  884/1000 Cost : 0.275186\n",
      "Epoch  886/1000 Cost : 0.274363\n",
      "Epoch  888/1000 Cost : 0.273544\n",
      "Epoch  890/1000 Cost : 0.272730\n",
      "Epoch  892/1000 Cost : 0.271920\n",
      "Epoch  894/1000 Cost : 0.271116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  896/1000 Cost : 0.270316\n",
      "Epoch  898/1000 Cost : 0.269522\n",
      "Epoch  900/1000 Cost : 0.268733\n",
      "Epoch  902/1000 Cost : 0.267950\n",
      "Epoch  904/1000 Cost : 0.267174\n",
      "Epoch  906/1000 Cost : 0.266404\n",
      "Epoch  908/1000 Cost : 0.265640\n",
      "Epoch  910/1000 Cost : 0.264883\n",
      "Epoch  912/1000 Cost : 0.264134\n",
      "Epoch  914/1000 Cost : 0.263392\n",
      "Epoch  916/1000 Cost : 0.262658\n",
      "Epoch  918/1000 Cost : 0.261932\n",
      "Epoch  920/1000 Cost : 0.261214\n",
      "Epoch  922/1000 Cost : 0.260505\n",
      "Epoch  924/1000 Cost : 0.259805\n",
      "Epoch  926/1000 Cost : 0.259115\n",
      "Epoch  928/1000 Cost : 0.258434\n",
      "Epoch  930/1000 Cost : 0.257763\n",
      "Epoch  932/1000 Cost : 0.257103\n",
      "Epoch  934/1000 Cost : 0.256453\n",
      "Epoch  936/1000 Cost : 0.255814\n",
      "Epoch  938/1000 Cost : 0.255187\n",
      "Epoch  940/1000 Cost : 0.254571\n",
      "Epoch  942/1000 Cost : 0.253966\n",
      "Epoch  944/1000 Cost : 0.253374\n",
      "Epoch  946/1000 Cost : 0.252794\n",
      "Epoch  948/1000 Cost : 0.252226\n",
      "Epoch  950/1000 Cost : 0.251671\n",
      "Epoch  952/1000 Cost : 0.251129\n",
      "Epoch  954/1000 Cost : 0.250600\n",
      "Epoch  956/1000 Cost : 0.250083\n",
      "Epoch  958/1000 Cost : 0.249580\n",
      "Epoch  960/1000 Cost : 0.249089\n",
      "Epoch  962/1000 Cost : 0.248612\n",
      "Epoch  964/1000 Cost : 0.248147\n",
      "Epoch  966/1000 Cost : 0.247695\n",
      "Epoch  968/1000 Cost : 0.247255\n",
      "Epoch  970/1000 Cost : 0.246828\n",
      "Epoch  972/1000 Cost : 0.246412\n",
      "Epoch  974/1000 Cost : 0.246009\n",
      "Epoch  976/1000 Cost : 0.245616\n",
      "Epoch  978/1000 Cost : 0.245235\n",
      "Epoch  980/1000 Cost : 0.244864\n",
      "Epoch  982/1000 Cost : 0.244503\n",
      "Epoch  984/1000 Cost : 0.244152\n",
      "Epoch  986/1000 Cost : 0.243810\n",
      "Epoch  988/1000 Cost : 0.243477\n",
      "Epoch  990/1000 Cost : 0.243152\n",
      "Epoch  992/1000 Cost : 0.242834\n",
      "Epoch  994/1000 Cost : 0.242523\n",
      "Epoch  996/1000 Cost : 0.242219\n",
      "Epoch  998/1000 Cost : 0.241921\n",
      "Epoch 1000/1000 Cost : 0.241629\n"
     ]
    }
   ],
   "source": [
    "#optimizer 설정\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr= 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    #H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    #cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "    \n",
    "    #cost 로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #20번마다 로그 출력\n",
    "    if epoch %2 ==0:\n",
    "        print('Epoch {:4d}/{} Cost : {:6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sh_python",
   "language": "python",
   "name": "sh_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
